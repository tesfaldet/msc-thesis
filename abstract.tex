\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{1}
This thesis introduces a two-stream model for dynamic texture synthesis.
The model is based on pre-trained convolutional networks (ConvNets)
that target two independent tasks: (i) object recognition, and (ii)
optical flow prediction.
Given an input dynamic texture, statistics
of filter responses from the object recognition ConvNet
encapsulate the per-frame appearance of the input texture, while
statistics of filter responses from the optical flow ConvNet model
its dynamics.
To generate a novel texture, a randomly initialized input sequence is optimized
to match the feature statistics from each
stream of an example texture.  
Inspired by recent work on image style transfer and enabled by the
two-stream model, the synthesis approach is applied to combine the
texture appearance from one texture with the dynamics of another 
to generate entirely novel dynamic textures.
The proposed approach generates novel, high quality samples 
that match both the framewise appearance and temporal evolution
of input texture.
Finally, a quantitative evaluation of the proposed dynamic texture synthesis approach is performed via a thorough user study.
\end{abstract}