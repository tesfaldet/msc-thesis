\chapter{Conclusion}

\section{Thesis summary}\label{sec:thesis_summary}

This thesis presented a novel, two-stream analysis
of dynamic textures using ConvNets to independently represent appearance and
dynamics statistics, culminating in four primary contributions to the dynamic texture literature spanning both theory and application.

First, theoretical insight into the characterization of dynamic textures is provided by building a novel factored representation of both appearance and dynamics. Second, for the representation of dynamics, a novel ConvNet based on the ``marginalized'' spacetime-oriented energy model of Derpanis and Wildes \cite{derpanis2012spacetime} was constructed. It was shown to provide a substantial improvement on the temporal coherence of synthesized dynamic textures when compared to using a dynamics representation based purely on optical flow. Third, a novel form of style transfer was demonstrated, where the appearance and dynamics information from different texture sources are combined to produce a compelling composition of the two. This capability was shown to be enabled by the factored two-stream representation of appearance and dynamics. Finally, the model was applied to a variety of dynamic texture synthesis tasks and it was shown that, so long as the input textures followed the thesis' assumptions of a dynamic texture, \ie, have spatially invariant statistics and spatiotemporally invariant dynamics, the resulting synthesized textures were compelling. This point was validated both qualitatively and quantitatively through a large user study and comparisons with extant methods for dynamic texture synthesis.

Beyond the theoretical implications of this thesis lie numerous applications in the creative-industry including, but not limited to, computer-generated imagery, digital painting, and image editing. More broadly, the ability 
to animate static imagery via dynamics style transfer can meaningfully contribute 
to the emerging artistic medium of computer-generated art.

\section{Future work}\label{sec:future_work}

Consequently, a few limitations were revealed; however, in light of the implications of some of the experiments (\eg, dynamics style transfer), potential avenues for artistic exploration have been revealed as well. These have been left as directions for future work. This section will first describe the aforementioned limitations and propose possible solutions, then it will outline some interesting potential artistic applications as well as some extensions to the model to enable these applications.

First, much like has been reported in recent image style transfer
work \cite{gatys2016image}, results in this thesis show that high frequency
noise and chromatic aberrations are a problem in generation. Another issue that arises is when the model fails to capture
textures with spatially-varying appearance, (\eg, 
\texttt{flag} in Fig.\ \ref{fig:failures}) and
spatially-inconsistent dynamics (\eg, \texttt{escalator} in 
Fig.\ \ref{fig:failures}).
By collapsing the local statistics into a Gram matrix, 
the spatial and temporal pattern organization is lost.
Simple post-processing methods, \eg, blurring, may alleviate issues with noise and chromatic aberration, but holistically, these appearance-based issues point to a need for a better representation of appearance. To preserve spatial structure, Berger \etal \cite{berger2016} experimented with incorporating long range consistency in ConvNet-based texture synthesis by computing multiple Gram matrices at a layer instead of one. Specifically, rather than computing correlations between activations at a single spatial position, correlations between activations across spatial positions were computed as well. Although this approach seems promising, it is worth noting that it is computationally expensive due to the additional Gram computations.

The multiscale distributed representation of dynamics in the dynamics stream was implicitly learned through the proxy task of optical flow regression. Although optical flow was sufficient to produce spacetime-oriented filters, it is unclear how its limitations in modelling complex dynamics affects the dynamics modelling capacity of the learned filters. The abundance of optical flow groundtruth was a motivating factor for its usage; however, for future work, it would be interesting to compare with other, seemingly more suitable, proxy tasks, such as dynamic texture recognition \cite{derpanis2012spacetime}.

The proposed two-stream model used a learned approach for instantiating the filter weights of the MSOE dynamics representation \cite{derpanis2012spacetime}. Although most of the synthesized dynamic textures appeared to be perceptually similar to their targets, it remains to be seen how the learned dynamics representation compares to a handcrafted dynamics representation \cite{derpanis2012spacetime}. An advantage of a handcrafted approach is having analytically-defined oriented filters that do not need to be learned through a possibly noisy proxy task. It is not clear if using handcrafted filters would improve results, however, it is worth comparing against in future work. Likewise, interesting comparisons could be made with using a handcrafted, rather than learned, appearance stream representation via spatially-oriented energy filters, \eg, 2D Gaussian $n$-th derivative oriented filters.

The user study quantitatively compared the final two-stream model with a baseline model. Although a qualitative comparison was made with previously proposed approaches for dynamic texture synthesis \cite{funke2017,xie2017synthesizing}, the user study did not include a comparison with these approaches. The addition of a quantitative comparison between the two-stream model and the extant approaches could further illustrate the advantages of the two-stream model that were outlined in the qualitative comparison. Additionally, it would be interesting to quantitatively compare with the handcrafted approach \cite{derpanis2012spacetime} mentioned in the previous paragraph.

Due to GPU memory limitations, the temporal extent of the learned spacetime-oriented filters in the first layer of the dynamics stream was restricted to $T=2$. In spite of this limitation, the two-stream model still managed to synthesize impressive results. However, a small temporal extent limits the range of temporal frequencies that can be captured, thus limiting the range of dynamics that can be modelled. For future work, it would be interesting to investigate the relationship between the quality of synthesized dynamic textures and the temporal extent of the filters in the first layer of the dynamics stream.

ConvNet-based texture synthesis models use a Euclidean metric (\ie, the Frobenius norm) for measuring the distance between the Gram matrices of the target and synthesized textures. The Gram matrix is a positive semidefinite matrix, existing in a non-Euclidean space. Thus, non-Euclidean metrics may be more suitable to use. For example, the log-Euclidean \cite{arsigny2006} metric provides many benefits over the Frobenius norm; notably, it is scale-invariant and it defines a minimal geodesic distance on the manifold of positive semidefinite matrices. Minimal geodesic distances are useful in applications for smoothly interpolating between two positive semidefinite matrices, \eg, Gram matrices. For texture synthesis, this may translate to higher quality intermediately-synthesized textures during optimization, and possibly faster convergence.

Beyond addressing these limitations, a natural next step would be
to extend the idea of a factorized representation into feed-forward
generative networks that have found success in static image
synthesis, \eg, \cite{johnson2016,ulyanov2016}. These networks move the computational burden of the optimization process to a learning stage, where given a single example of a texture, a compact ``generator'' ConvNet is trained to generate multiple samples of the same texture. The same texture-modelling ConvNet used before is kept as the ``perceptual'' loss, measuring similarity in activation statistics between the synthesized and target textures. These networks have shown to be hundreds of times faster than the traditional approach introduced by Gatys \etal \cite{gatys2015}, on which the two-stream model is based.

Dynamics style transfer is an exciting application of the two-stream model that encourages further exploration on texture-based artistic tools. Like image style transfer \cite{gatys2016image}, however, it is limited in its ability to allow artists granular control over visual aesthetics. Recently, there has been some progress extending image style transfer to include control over spatial location and colour information across spatial scales \cite{gatys2017}. An extension to dynamics style transfer could involve incorporating this technique to allow artists to decide which (textured) regions of an image to transfer dynamics to. A metaphorical ``motion brush''.