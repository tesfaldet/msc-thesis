\chapter{Background}\label{chap:background}

This chapter aims to summarize the relevant theory and mathematics of
convolutional networks,
static and dynamic texture synthesis, image style transfer, and representations of dynamics so as to provide sufficient
background information for the following chapters. Research
related to this thesis is covered simultaneously.

\section{Convolutional networks}

This section provides a brief summary of convolutional networks; a more thorough overview can be found in texts (\eg, \cite{goodfellow2016}) and review articles (\eg, \cite{hadji2018}).

A convolutional network (ConvNet) is a feed-forward computational graph of processing nodes for approximating non-linear functions. It is commonly used in analyzing visual imagery and is a class of artificial neural networks (ANNs), which are computational systems inspired by biological neural networks. ConvNets perform tasks (\eg, classifying objects or regressing the angle of rotated handwritten digits) by processing an input, $\mathbf{X} \in \mathbb{R}^{H_x \times W_x \times C_x}$, in a feed-forward manner via a series of linear and non-linear transformations and producing an output, $\mathbf{Y} \in \mathbb{R}^{H_y \times W_y \times C_y}$, relevant to the task (such as the class of an object). Here, $H_\ast \times W_\ast$ represents the spatial dimensions and $C_\ast$ represents the number of channels (\ie, the ``depth'' of the input). For example, a $3$-channel colour image input with $256 \times 256$ spatial locations is represented as $\mathbf{X} \in \mathbb{R}^{256 \times 256 \times 3}$, where each spatial location contains three pixel intensities, corresponding to the amount of intensity of the colours Red, Green, and Blue (RGB), respectively. Each non-linear transformation acts as a point of demarcation in the network known as a \emph{layer}. ConvNets typically consist of multiple layers, each containing a collection of nodes sometimes called \emph{neurons}. At each spatial-channel location of the input to a layer, a neuron computes local non-linear transformations, \eg, $\bar{\phi}(\mathbf{x}) = \max{(0, \phi(\mathbf{x}))}$ (known as the \emph{rectified linear unit} or ReLU \cite{nair2010rectified}). At a single location of the input, $\mathbf{x} \equiv (x, y, z)$, the non-linear transformation performed by this neuron produces an output called a \emph{feature activation}, $\bar{\phi}(\mathbf{x}) \in \mathbb{R}$. The set of activations produced by a neuron at every location of the input is known as an \emph{activation} or \emph{feature map}, $\bar{\phi} \in \mathbb{R}^{H_{\bar{\phi}} \times W_{\bar{\phi}} \times C_{\bar{\phi}}}$. At the $l$-th layer of the network and for each location $\mathbf{x}$ of the input map, the input to each neuron is the weighted linear combination of activations from local, neighbouring neurons at the previous layer, $l-1$:
\begin{equation}
	\begin{aligned}
		\phi^l(\mathbf{x}) &= \left(\mathbf{w}^l * \bar{\phi}^{l-1}(\mathbf{x})\right) + b^l\\
		&= \left(\sum_{(i, j, k) \in \Omega} \mathbf{w}^l(i, j, k) \bar{\phi}^{l-1}(x - i, y - j, z - k)\right) + b^l\ ,
	\end{aligned}
\end{equation}
where $\mathbf{w}^l$ are the \emph{weights} (or \emph{filter}) applied to the input $\bar{\phi}^{l-1}(\mathbf{x})$, $\Omega$ is a spatial-channel neighbourhood centered about $\mathbf{x}$, $b^l$ is an offset term known as the \emph{bias}, and $\ast$ represents the \emph{convolution} (or \emph{filtering}) operator. Colloquially, the term ``convolution'' is often used to describe the combined process of convolving over an input and subsequently computing its activation. Convolutions are performed at each location of the input, operating on a localized area of influence called its \emph{receptive field}. ConvNets are distinguished from most other ANNs in that the weights associated with a convolution are shared across the entire input. Specifically, when convolving over an input, instead of using a unique filter for each location, the same filter is reused---this is called \emph{weight sharing}. The idea is that if a filter is important for capturing a feature (\eg, edge, corner, face, etc.) at a particular location, then it is assumed that the filter is important at other locations as well since the same feature may appear elsewhere.
At the base of the ConvNet, inputs are typically images while inputs at intermediate layers are activation maps (\eg, $\bar{\phi} \in \mathbb{R}^{H_{\bar{\phi}} \times W_{\bar{\phi}} \times C_{\bar{\phi}}}$). 

\subsubsection{Pooling}

ConvNets commonly include \emph{pooling} layers that combine regions of activations into a single activation in the next layer. There are two commonly used types of pooling: \emph{average pooling}, which uses the average value from each of region of activations at the previous layer, and \emph{max pooling}, which uses the max value instead. Pooling provides a degree of translation-invariance to ConvNets by making them less affected by small changes in the positions of input activations. It is typically followed by a downsample operation to reduce spatial resolution.

\subsubsection{Normalization}

Widely used in ConvNets are \emph{normalization} layers that serve to inhibit or bind local activations to a certain range. Normalization is typically done across channels, such as with the commonly used \emph{divisive normalization}:
\begin{equation}
	\hat{\phi}^l_i(\mathbf{x}) = \frac{\bar{\phi}^l_i(\mathbf{x})}{\sum_{j=1}^{C}{\bar{\phi}^l_j(\mathbf{x})} + \epsilon}\ ,
\end{equation}
where $\bar{\phi}^l_i(\mathbf{x})$ is an activation from the $i$-th channel at spatial position $\mathbf{x}$, $C$ is the total number of channels, and $\epsilon$ is a small value to prevent division by zero.
An issue with using unbounded activation functions (\eg, ReLU) with convolutions in a ConvNet is that their outputs keep increasing with increasing contrast of the input. This dependency on contrast magnitude makes it difficult to determine whether a high response is indicative of a salient feature or high input contrast. Thus, divisive normalization binds unbounded activations across channels such that information about the magnitude of contrast is discarded in favour of a representation based only on relative variations in contrast across input activations, \ie, the underlying feature patterns of the input.

\subsubsection{Training a convolutional network}

ConvNets ``learn'' to perform tasks through an iterative process called \emph{training}, which involves optimizing their weights based on an objective over training data (\eg, input images with corresponding expected outputs). At each iteration, an input is fed through the network to produce an output that is subsequently evaluated against the expected, \ie, groundtruth, output for the given input. This evaluation is known as the \emph{loss function} and represents the network's performance on the task. Implicitly, it also represents the objective the network must achieve, \eg, minimizing classification or regression error. Starting from the loss, the network adjusts its weights and biases at each layer via the gradient of the loss with respect to the weights and biases at that layer. The adjustment of weights and biases via the gradient of the loss function is called \emph{backpropagation} \cite{goodfellow2016}. After a suitable amount of training iterations, this \emph{gradient descent} process is terminated.

\subsection{Two-stream convolutional networks}

In the context of video analysis, two-stream ConvNets \cite{simonyan2014,feichtenhofer2016two} are a class of ConvNets that separate processing of input temporal imagery into two recognition streams (spatial and temporal), each with its own task. They are inspired by the two-stream hypothesis \cite{goodale1992} that models the human visual cortex in terms of two pathways, the ventral stream (involved with object recognition) and the dorsal stream (involved with motion processing).

Both streams are implemented as ConvNets with the overall intent to decouple processing of spatial and temporal information. This decoupling allows each network to be trained on separate tasks, \eg, the spatial ConvNet can be trained on object recognition and the temporal ConvNet can be trained on motion recognition. Furthermore, the decoupled spatial ConvNet can take advantage of the availability of large amounts of annotated image data for training, \eg, the ImageNet dataset \cite{russakovsky2015}. Two-stream ConvNets have been successfully used for video understanding tasks in computer vision, with particular attention to action recognition \cite{simonyan2014,feichtenhofer2016two}.

\section{Parametric texture synthesis}

Texture synthesis is the process of algorithmically constructing a texture that
matches or extends a given source texture by taking advantage of its structural 
content. There are two general approaches that have dominated the texture
synthesis literature: non-parametric sampling approaches that
synthesize a texture by sampling pixels of a given source texture
\cite{efros1999,kwatra2003graphcut,schodl2000,wei2000}, and 
statistical parametric models that aim to synthesize a texture by sampling
from a parameterized model of the source texture.
As the proposed approach is an instance of a parametric model, this section 
will focus on these parametric approaches.

The statistical characterization of visual textures was introduced
in the seminal work of Julesz \cite{julesz1962}.
He conjectured that particular statistics of pixel intensities
were sufficient to partition textures into metameric (\ie,
perceptually indistinguishable) classes. 
Later work leveraged this notion for static texture synthesis
\cite{heeger1995pyramid,portilla2000parametric}.
In particular, inspired by models of the early stages of visual 
processing, statistics of (handcrafted) multi-scale oriented filter 
responses were used to optimize an initial noise pattern 
to match the filter response statistics of an input texture.

More recently, Gatys \etal \cite{gatys2015} demonstrated
impressive results by replacing the handcrafted linear filter bank with the learned filters from the VGG-19
\cite{simonyan2014very} ConvNet pre-trained on the ImageNet \cite{russakovsky2015} dataset for the task of object
recognition.
Textures were modelled in terms of the normalized correlations between activation maps within several layers of the network.

\subsection{Texture synthesis using a convolutional network}
\label{sec:texture_synthesis_using_a_convnet}

Since the two-stream approach to dynamic texture synthesis proposed in this thesis is an extension of the Gatys \etal \cite{gatys2015} texture 
synthesis model, it is useful to describe their approach here. Given a target texture as input,
let $\mathbf{A}^{l} \in \mathbb{R}^{N_l\times M_l}$
be its row-vectorized activation maps at the $l$-th layer of a ConvNet, where $N_l$ and $M_l$ denote the number of
activation maps and the number of spatial locations,
respectively (in the case of Gatys \etal \cite{gatys2015}, they used the VGG-19 ConvNet, and they normalized the network by scaling its weights such that the mean activation of each convolutional filter over images and positions is equal to one).
The normalized correlations between activation maps
within a layer are encapsulated by a Gram matrix,
$\mathbf{G}^l \in \mathbb{R}^{N_l \times N_l}$, whose entries are given by:
\begin{equation}
	G_{ij}^l = \frac{1}{N_l M_l} \sum_{k=1}^{M_l} A_{ik}^l A_{jk}^l\ ,
\end{equation}
where $A_{ik}^l$ denotes the activation of feature $i$ at
location $k$ in layer $l$ on the target texture.
Given a synthesized texture as input, similarly, let its row-vectorized activation maps
be $\hat{\mathbf{A}}^{l} \in \mathbb{R}^{N_l\times M_l}$ and its normalized
activation map correlations be the Gram matrix, $\hat{\mathbf{G}}^l \in \mathbb{R}^{N_l \times N_l}$, whose entries are given by:
\begin{equation}
	\hat{G}_{ij}^l = \frac{1}{N_l M_l} \sum_{k=1}^{M_l} \hat{A}_{ik}^l \hat{A}_{jk}^l\ .
\end{equation}
The final objective is defined as the average of the mean squared error between
the Gram matrices of the target texture and that of the synthesized texture:
\begin{equation}
   \mathcal{L} = \frac{1}{L} \sum_{l} \Vert \mathbf{G}^l - \hat{\mathbf{G}}^l \Vert^2_F\ ,
   \label{eq:tex_loss}
\end{equation}
where $L$ is the number of ConvNet layers used when computing Gram matrices
and $\Vert \cdot \Vert_F$ is the Frobenius norm. In the case of Gatys \etal \cite{gatys2015}, Gram matrices were computed on
layers \emph{conv1\_1}, \emph{pool1}, \emph{pool2}, \emph{pool3}, and \emph{pool4}. Through systematic evaluation, they reported these layers to be qualitatively superior to other layer subsets for texture synthesis.
To note, the Gram matrix is positive semidefinite and thus exists in a non-Euclidean space. Non-Euclidean metrics (\eg, the Log-Euclidean \cite{arsigny2006}) may be more suitable than the Frobenius norm, however, following Gatys \etal's \cite{gatys2015} implementation, the Frobenius norm is used here instead. Gatys \etal did not investigate using other metrics, so this may be worth exploring for future work.

Before synthesizing a texture, an initial forward pass through the ConvNet is
performed with the target texture as input. The target texture's Gram matrices
across various layers in the network are computed and stored to be used in the final objective for
the synthesis process, Eq.\ \ref{eq:tex_loss}. Then the synthesized texture is initialized with Independent and Identically Distributed (IID)
Gaussian noise. The final
objective, Eq.\ \ref{eq:tex_loss}, is minimized with respect to the synthesized texture. With each
iteration of the optimization process, the pixel values of the synthesized texture are updated to appear increasingly
perceptually similar to the target texture. An overview of this process is presented in Fig.\ \ref{fig:vgg_texture_synthesis}.
\clearpage
\input{fig_vgg_texture_synthesis}
\clearpage

\subsection{The Gram matrix as a texture metric}

Before explaining the Gram matrix as a suitable texture metric, it is necessary to first understand the mathematics behind it. The Gram matrix, $\mathbf{G} \in \mathbb{R}^{n \times n}$, of a set of $m$-dimensional vectors, $v_1, \dots , v_n \in \mathbb{R}^m$, is the symmetric matrix of inner products, whose entries are given by $G_{ij} = \langle v_i, v_j \rangle$. Essentially, the Gram matrix is a covariance matrix describing which of its input vectors are correlated with each other. In the case of Gatys \etal's \cite{gatys2015} texture synthesis with a ConvNet, the set of vectors used to compute the Gram matrix are row-vectorized activation maps (Fig.\ \ref{fig:gram_matrix}).
\input{fig_gram_matrix}

The hierarchical feature representations learned by ConvNets have been shown to be powerful for difficult visual perceptive tasks such as object recognition \cite{krizhevsky2012,simonyan2014very}, significantly outperforming previous models that have relied on handcrafted approaches. Significantly, Movshon and Simoncelli \cite{movshon2014} have suggested that given the texture-like features that can be captured in the intermediate stages of these ConvNets, images synthesized from these representations may prove useful as stimuli in perceptual or physiological investigations. This suggests that ConvNets may be a suitable basis for modelling textures, and consequently, texture synthesis.

In texture synthesis, the Gram matrix computed on activations measures the amount that co-located features tend to activate together. It transforms the spatially-varying feature space into a stationary, spatially-invariant one. Since textures exhibit stationary statistics (\ie, the visual content is spatially homogeneous), by computing Gram matrices across several layers, a stationary, multi-scale representation of the input image in terms of its texture information is achieved.

\subsection{Image style transfer}

In subsequent work, Gatys \etal's \cite{gatys2016image} texture model was used in image style
transfer, where the style of one image was
combined with the image content of another to produce a new image.
This was achieved by appending an additional term to Eq.\ \ref{eq:tex_loss}
that enforced the synthesized texture to match the semantic content of the given content image. Specifically,
\begin{equation}
   \mathcal{L} = \frac{1}{L_{style}} \sum_{l} \Vert \mathbf{G}^l - \hat{\mathbf{G}}^l \Vert^2_F\ + \frac{1}{L_{content}} \sum_{l} \Vert \mathbf{A}^l - \hat{\mathbf{A}}^l \Vert^2_F\ ,
   \label{eq:styletransfer_loss}
\end{equation}
where $L_{style}$ and $L_{content}$ are the number of VGG-19 layers used when computing Gram matrices and activation maps, respectively. Gram matrices are computed on the same layers as before, and activation maps are computed on layer \emph{conv4\_2}.

To briefly review, the Gram matrix of activation maps conveys a notion of texture, or ``style'', describing which features tend to activate together. Although the style of the style image is preserved, the global arrangement of its features are not. By including the objective of matching the features of the content image, however, the global arrangement of semantic image content from the content image is preserved. This results in a synthesized image that contains the content of the content image and the style of the style image. The idea of transferring style from one image to another serves as a loose inspiration for the novel dynamics style transfer enabled by the proposed two-stream model. Although with dynamics style transfer, content is not preserved, as it is only a transfer of texture dynamics encompassed by a texture synthesis process.

\section{Dynamic texture synthesis}

Dynamic textures extend from static textures with an additional temporal dimension. The stationarity of spatial statistics of static textures also applies to the temporal domain of dynamic textures.

Unlike static texture synthesis, dynamic texture synthesis has not been as deeply explored. Loosely related---although tangential to dynamic texture synthesis and the proposed dynamics style transfer---Ruder \etal \cite{ruder2016} extended the image style transfer model of Gatys \etal \cite{gatys2016image} to video by using 
optical flow to enforce temporal consistency of the
resulting imagery. Although their model produced a video output, their core approach focused on an analysis of static style on a per-frame basis. This is tangential to the proposed approach since dynamic textures require an analysis across space \emph{and time}.

Variants of linear autoregressive models have been studied
\cite{szummer1996,doretto2003,wang2003,fitzgibbon2001} that jointly model the appearance and
dynamics of spatiotemporal patterns.
More recent work has considered ConvNets as a basis for modelling 
dynamic textures.
Xie \etal \cite{xie2017synthesizing} proposed a spatiotemporal
generative model where each dynamic texture is modelled as a random
field defined by multiscale, spatiotemporal ConvNet filter responses
and dynamic textures are realized by sampling the model.
Unlike the proposed approach, which assumes pre-trained fixed networks,
Xie \etal's \cite{xie2017synthesizing} approach requires their ConvNet weights to be trained using the input texture prior to synthesis. The manner in which they model dynamic textures appears to limit synthesis to a reconstruction, not an extrapolation, of the original sequence, limiting the generalizability of their approach, \eg, synthesizing textures beyond the spatiotemporal extent of the input.
A recent unpublished work by Funke \etal \cite{funke2017} described preliminary 
results extending the framework of Gatys \etal \cite{gatys2015} 
to model and  synthesize dynamic textures by computing a Gram 
matrix of filter activations over a small spatiotemporal window.
In contrast, the proposed two-stream filtering architecture is more 
expressive as the dynamics stream is specifically tuned to 
spatiotemporal dynamics.
Moreover, the factorization
in terms of appearance and dynamics enables a novel form of
style transfer, where the dynamics of one pattern are 
transferred to the appearance of another to generate an
entirely new dynamic texture.
This work is the first to demonstrate this form of style transfer.


\section{Representations of dynamics}

Numerous representations of dynamics in temporal imagery have been explored, each with their own limitations and level of abstraction. Figure \ref{fig:dynamics_representations} illustrates an organization of several extant representations of temporal imagery dynamics.
\input{fig_dynamics_representations}
At one extreme, no commitment to an abstraction is made, the raw pixelwise intensity is used directly. This representation fails to leverage the rich underlying structure in the data. The remaining representations are discussed below.

\subsection{Optical flow}

At the other extreme of Fig.\ \ref{fig:dynamics_representations}, a two-dimensional (2D) vector field is used to represent the dynamics of the input temporal imagery. This vector field is known as \emph{optical flow}. It is used to represent the apparent motion of image pixels between two consecutive frames that is caused by the movement of objects or the camera. Each vector in the 2D vector field represents a displacement consisting of a horizontal and vertical component, describing the movement of pixels from one frame to the next. Figure \ref{fig:optical_flow} provides a visualization of optical flow.
\input{fig_optical_flow}
The recovery of optical flow from temporal imagery has long been studied in computer vision. Traditionally, it has been addressed by handcrafted approaches \eg, \cite{horn1981,lucas1981,revaud2015epicflow}. Recently, ConvNet approaches have been demonstrated as viable alternatives \cite{dosovitskiy2015,ilg2017,ranjan2017,yu2016}.

A limitation of optical flow is its reliance on a single coherent movement for each pixel and its underlying assumption on brightness constancy, which is difficult to justify for the spectrum of dynamics one encounters in the real world. Examples of dynamics that optical flow fails to capture include flickering, semi-transparent motion, and stochastic dynamics. These are some of the dynamics typically exhibited by dynamic textures. Therefore, optical flow is not a suitable substrate for representing the spectrum of dynamics in dynamic textures.

\subsection{Marginalized spacetime oriented energies}
\label{sec:msoe}

Between the two extremes lies the representation of dynamics that aims to capture a distribution of measurements of spacetime orientations in the input temporal imagery.
Unlike flow-based analyses that focus on the apparent motion (\ie, translation) present in the data, measurements of spacetime orientations take a geometric and generalized approach in capturing \emph{spacetime structures}: oriented structures in the spatiotemporal domain that manifest themselves as motion or non-motion, \eg, flickering, stochastic dynamics, etc.

Previous work \cite{adelson1985spatiotemporal,fahle1981,heeger1988,simoncelli1998,watson1983,nishimoto2011,derpanis2012spacetime} has shown that the velocity of image content (\ie, motion) can be interpreted as a 3D oriented structure in the $x$-$y$-$t$ spatiotemporal domain. Furthermore, in the frequency domain, the signal energy of these oriented structures lie on a plane through the origin where the slant of the plane is defined by the velocity of the image content.
For example, in the case where a spacetime structure is defined by the image velocity $(u, v)^\top$, the unit normal of the plane is given by $\hat{\mathbf{n}}=(u, v, 1)^\top / ||(u, v, 1)^\top||$.
Hence, energy models of visual motion, like those presented in these works, are described as ``oriented energy'' or ``motion energy'' models, and they attempt to identify this orientation-plane (and hence the pattern's velocity) via a set of image filtering operations. Specifically, given an input image sequence, these models consist of an alternating sequence of linear and non-linear operations that yield a distributed representation (\ie,  implicitly coded) of pixelwise
optical flow. These models have been motivated and studied in a variety of contexts, including computer vision, visual neuroscience, and visual psychology

Whereas motion indicates a single, dominant orientation in the spatiotemporal and frequency domains, non-motion can indicate an unconstrained, underconstrained, multi-dominant, heterogeneous, or isotropic orientation. For example, an unconstrained orientation corresponds to structure-less imagery (\eg, image of a clear sky) in the spatiotemporal domain and point energy response in the origin in the frequency domain; and a multi-dominant orientation corresponds to multiple, super-imposed spacetime structures in the spatiotemporal domain (\eg, waterfall over a stationary background) and multiple, super-imposed oriented planes in the frequency domain. These, along with the other orientations, are visualized in Fig.\ \ref{fig:spacetime_structures}.

This thesis adopts the Marginalized Spacetime Oriented Energy (MSOE) approach of Derpanis and Wildes \cite{derpanis2012spacetime} in representing the observed distribution of dynamics (\ie motion and non-motion) of an input dynamic texture. They conjectured that the constituent spacetime orientations for a spectrum of common
visual patterns (\eg, dynamic textures) can serve as a basis for describing the temporal
variation of an image sequence. They successfully applied their model for the task of dynamic texture recognition; here it is used for the task of dynamic texture synthesis. Significantly, a completely analytically-defined
oriented energy ConvNet model provides the current state-of-the-art
for the related task of dynamic texture recognition \cite{hadji2017}. The proposed two-stream architecture adopts the MSOE model by encoding it as a ConvNet that serves as the representation of observed dynamics of input dynamic textures---the dynamics stream. The same computational steps are used, however, the handcrafted filters of the MSOE model are not used and are learned instead so that they are better tuned to deal with the noise distributions encountered in natural imagery. The construction of the ConvNet is discussed in the next chapter and the MSOE model is reviewed here.

\clearpage
\input{fig_spacetime_structures}
\clearpage

Given input temporal imagery, $\mathbf{I} \in \mathbb{R}^{T \times H \times W}$ (time $\times$ height $\times$ width), a bank of oriented 3D
filters, \eg, Gaussian third derivative filters $G_3 \in \mathbb{R}^{T \times H \times W}$, which are sensitive to a range of
spatiotemporal orientations, are each applied:
\begin{equation}
	E_{\hat{\theta}} = G_{3_{\hat{\theta}}} \ast \mathbf{I}\ ,
\end{equation}
where $\ast$ denotes convolution, and $G_{3_{\hat{\theta}}}$ is a Gaussian third derivative filter oriented in the direction of the 3D unit vector $\hat{\theta}$ which lies along the filter's symmetry axis. Each of these filtering operations results in a spacetime volume of filter responses, $E_{\hat{\theta}}$.
These filter responses are then rectified (squared) and
pooled over local spacetime regions to make the responses robust
to the phase of the input signal, \ie, robust to the
alignment of the filter with the underlying image
structure:
\begin{equation}
	\bar{E}_{\hat{\theta}} = \sum_{(x, y, t) \in \Omega}{{E_{\hat{\theta}}(x, y, t)}^2}\ .
\end{equation}
At this point, each oriented energy measurement includes measurement of spatial orientation. This means that spatial image structures will affect the responses of the bank of oriented filters, making them dependent on spatial appearance. This dependence is unwanted as it can occur at an otherwise coherent dynamic region, \eg, a surface with varying spatial appearance exhibiting a single, dominant motion. Thus, a description consisting purely of pattern dynamics is sought.
To remove this difficulty, the spatial orientation component of each filter is discounted via ``marginalization''. Specifically, filter responses consistent with the same temporal orientation (not necessarily the same spatial orientation), $\hat{\theta}_i$, are summed:
\begin{equation}
	E_{\hat{\mathbf{n}}} = \sum_{i = 1}^{N}{E_{\hat{\theta}_i}}\ ,
	\label{eq:oriented_filter_2.8}
\end{equation}
where $\hat{\mathbf{n}}$ denotes the unit normal of the plane in frequency-space that the spacetime structures captured by these filters lie upon (implicitly describing a single temporal orientation), and $N$ denotes the number of these filters.
These responses provide a pixelwise distributed measure
of which spacetime structures (discounting spatial information) are
present in the input.
However, these responses are confounded by local image
contrast that makes 
it difficult to determine
whether a high response is indicative of the presence of
a spacetime structure or simply due to high image
contrast.
To address this ambiguity, an $\textrm{L}_1$
normalization is applied across oriented filter responses which
results in a representation that is robust to local
appearance variations but highly selective to 
spacetime orientation:
\begin{equation}
	\hat{E}_{\hat{\mathbf{n}}_i} = \frac{E_{\hat{\mathbf{n}}_i}}{\sum_{j = 1}^{M}{E_{\hat{\mathbf{n}}_j}} + \epsilon} \ ,
\end{equation}
where $\hat{E}_{\hat{\mathbf{n}}_i}$ denotes an oriented filter response from Eq.\ \ref{eq:oriented_filter_2.8} corresponding to a plane in frequency-space with unit normal $\hat{\mathbf{n}}_i$, and $\epsilon$ is a small value (\eg, $1\mathrm{e}{-12}$) to avoid division by zero.